---
title: "HW1 - Data Science 2"
author: "Yuri Almeida Cunha"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include=FALSE}

# clear memory
rm(list=ls())

#Load necessary packages

library(tidyverse)
library(caret)
library(rpart.plot)
library(gbm)
library(xgboost)
library(caTools)
library(pROC)
library(viridis)
library(skimr)
library(MLeval)
library(h2o)
library(pander)

# Load necessary graph functions
source("D:/CEU/Data_Science_2/DS2_Yuri/HW1/codes/graph_functions.R")


```

# Exercise 1

```{r}

# Load Data
data <- as_tibble(ISLR::OJ)
skimr::skim(data)

```

## Quick analysis into the data-set

The dataset contains 2 factor variables, and 16 numeric ones , 1070 observations. 
Non-missing values found (can tangibly reduce prediction power), with that no cleaning steps or imputation methods are necessary.
Classification models can run smoothly without any data preparation.

Note: Due to the dataset size, huge amount of time to process the models, and the non-possibility to run the XGBOOST model with h2o; for this entire exercise, I've decided to use the package caret.Although it's possible to visualize some small differences into the code itself, the hyperparameters can be defined more less in the same way. 

## a) Create a training data of 75% and keep 25% of the data as a test set.

```{r , results='hide'}

# Create a training data of 75% and keep 25% of the data as a test set.

my_seed <- 19920828
set.seed(my_seed)
train_indices <- as.integer(createDataPartition(data$Purchase, 
                                                p = 0.75, list = FALSE))
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]

# Train a decision tree as a benchmark model.

fitControl <- trainControl(method = "cv", 
                           number = 5,
                           summaryFunction = twoClassSummary,
                           classProbs=TRUE,
                           verboseIter = TRUE,
                           savePredictions = TRUE)

# Hyperparameters: TrainControl

# method = "cv": Single K-fold cross-validation, no bootstrapping ("repeatedcv" re-sampling) needed.
# Dichotomous response variable is balanced in the dataset. (CH and MM - close number of occurences.)
# verboseIter = TRUE: Prints the training log.
# classProbs = TRUE: Show class probabilities.
# savedPredictions = TRUE: Save the hold-out predictions of each resample.
# summaryFunction = twoClassSummary: computes sensitivity, specificity and the area under the ROC curve

# Hyperparameters: CART (Method:  "rpart")

# Caret uses cross-validation to optimize the CART model hyperparameters. 
# The package can call rpart() function and train the model through cross-validation.

# To tune the complexity parameter(cp), set method = "rpart".
# To tune the maximum tree depth, set method = "rpart2"
# tuneLength: Tell how many models should be generated.
# Let it default in this case. (small amount of variable) 


set.seed(my_seed)
model_tree_benchmark<-train(
  Purchase~.,
  data=data_train,
  method="rpart",
  trControl=fitControl
)

```

```{r echo=FALSE}

pander(model_tree_benchmark$results)

```


Using cp or max_depth as tuning parameters generated pretty similar results. However, the cp tunned reduced the complexity of the model with less amount of features (in most cases). Easier to explain.

Taking a quick look at the results of the CART model, It's possible to mention that the ROC value achieved a substantial value (higher than 0.8 where the "perfect" model is 1.0), With that, the "trade-off" rates demonstrated pretty excellent as well (for the optimal classification-threshold-invariant): Sensitivity (True Positives / All real Positives) is higher than 85% and Specificity (True Negatives / All real Negatives) is also bigger than 70%.

## a) Plot the final model and interpret the result (using rpart and rpart.plot is an easier option).

```{r}

# Plot the final model and interpret the result (using rpart and rpart.plot is an easier option).

plot(model_tree_benchmark)
rpart.plot(model_tree_benchmark$finalModel)

```

Now, taking a look into the plotted graph and having those rates into consideration, We can visualized the main important variables in order (LoyalCH , ListPriceDiff, PriceDiff), their decision cutoff's. And the distribuition of the dataset under those specific characteristics and it's predict values for the following conditions. Ex: If a observation comes up with a LoyalCH lower than 0.48, the model predicts the purchase as "MM"

## b) Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation.


### Random Forest

```{r, results='hide'}

## Probability Forest

train_control <- trainControl(
  method = "cv",
  n = 5,
  classProbs = TRUE, 
  summaryFunction = twoClassSummary,
  savePredictions = TRUE,
  verboseIter = TRUE
)

tune_grid <- expand.grid(
  .mtry = c(2,3,4,5),
  .splitrule = "gini",
  .min.node.size = c(5, 10, 20)
)

# Hyperparameters: Random Forest (Method: ranger)

# mtry: Number of variables randomly sampled as candidates at each split.
# splitrule: 'Gini'  
# The ’impurity’ measure is the Gini index for classification, the variance of the responses for regression and the sum of test statistics
# min.node.size: Minimal node size. Default 1 for classification, 5 for regression, 3 for survival, and 10 for probability

# random forest
set.seed(my_seed)
model_rf <- train(Purchase~ .,
                  data = data_train,
                  method = "ranger",
                  trControl = train_control,
                  tuneGrid = tune_grid,
                  # The ’impurity’ measure is the Gini index for classification
                  importance = "impurity"
)



```

Best model mtry = 5, splitrule = gini and min.node.size = 20, ROC = 0.89

### Gradient Boosting Machine

```{r, results='hide'}

## Gradient Boosting Machine

gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 5, 7, 9), 
                        n.trees = 500, # high number of trees, small shrinkage
                        shrinkage = c(0.005, 0.01),
                        n.minobsinnode = c(2,3,4,5)) 

# Hyperparameters: Stochastic Gradient Boosting (Method: gbm)

# interaction.depth (Maximum nodes per tree) - number of splits it has to perform on a tree (starting from a single node).
# n.trees: (number of trees) Increasing N reduces the error on training set, but setting it too high may lead to over-fitting.
# shrinkage: Learning Rate - High Values (close to 1.0) poor performance (over-fitting), Small-ones (lower than 0.01) requires big n.trees
# n.minobsinnode: the minimum number of observations in trees' terminal nodes. Small samples - lower this setting to five or even three.

set.seed(my_seed)
model_gbm <- train(Purchase~ .,
                   data = data_train,
                   method = "gbm",
                   trControl = train_control,
                   tuneGrid = gbmGrid,
                   # will show you both WARNING and INFO log levels
                   verbose = TRUE
)


```

Best model n.trees = 500, interaction.depth = 3, shrinkage = 0.005 and n.minobsinnode = 3, ROC = 0.90

### XGBoost

```{r, results='hide'}

## XGBoost

# Hyperparameters: Stochastic Gradient Boosting (Method: gbm)

# nrounds: max number of boosting iterations
# max_depth: maximum depth of a tree
# eta: control the learning rate (Lower value / Larger value for nrounds - Less overfitting): Default: 0.3 
# gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. (Higher Value, + conservative)
# colsample_bytree: subsample ratio of columns when constructing each tree.
# min_child_weight: minimum sum of instance weight (hessian) needed in a child. (Higher Value, + conservative)
# subsample: subsample ratio of the training instance. 
# Setting it to 0.5 means that xgboost randomly collected half of the data instances to grow trees and this will prevent overfitting.


xgb_grid <- expand.grid(nrounds = 500,
                        max_depth = c(1, 3, 5, 7, 9),
                        eta = c(0.005, 0.01),
                        gamma = 0.01,
                        colsample_bytree = c(0.3, 0.5, 0.7),
                        min_child_weight = 1, # similar to n.minobsinnode
                        subsample = c(0.5))
set.seed(my_seed)
model_xgboost <- train(Purchase ~ .,
                       method = "xgbTree",
                       data = data_train,
                       trControl = train_control,
                       tuneGrid = xgb_grid)


```

Best model were nrounds = 500, max_depth = 3, eta = 0.005, gamma = 0.01, colsample_bytree = 0.7, min_child_weight = 1, ROC = 0.9

## c) Compare the performance of the different models. Is any of these giving significantly different predictive power than the others? 

```{r , results='hide' , warning=FALSE, message=FALSE, comment=FALSE}

# Compare the performance of the different models (if you use caret you should consider using the resamples function). 

resamples <- resamples(list("decision_tree_benchmark" = model_tree_benchmark,
                            "rf" = model_rf,
                            "gbm" = model_gbm,
                            "xgboost" = model_xgboost))
summary(resamples)

logit_models <- list()
logit_models[["decision_tree_benchmark"]] <- model_tree_benchmark
logit_models[["RF"]] <- model_rf
logit_models[["GBM"]] <- model_gbm
logit_models[["XGBoost"]] <- model_xgboost

CV_AUC_folds <- list()

for (model_name in names(logit_models)) {
  
  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$CH)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }
  
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                           "AUC" = unlist(auc))
}

CV_AUC <- list()

for (model_name in names(logit_models)) {
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}


```

```{r echo=FALSE}

pander(unlist(CV_AUC))

```


Yes, although the CART benchmark model is the easiest one to interpret, its AUC (for the optimal classification-threshold-invariant) results showed significantly lower than the other models (RF, GBM XGboost) with a slightly advantage to gbm.

## d) Choose the best model and plot ROC curve for the best model on the test set

```{r}

# Choose the best model and plot ROC curve for the best model on the test set

## ROC Plot with built-in package 
gbm_pred <-predict(model_gbm, data_test, type="prob")
colAUC(gbm_pred, data_test$Purchase, plotROC = TRUE)

## ROC plot with own function
data_test[,"best_model_no_loss_pred"] <- gbm_pred[,"CH"]

roc_obj_holdout <- roc(data_test$Purchase, data_test$best_model_no_loss_pred)

createRocPlot <- function(r, plot_name) {
  all_coords <- coords(r, x="all", ret="all", transpose = FALSE)
  
  roc_plot <- ggplot(data = all_coords, aes(x = fpr, y = tpr)) +
    geom_line(color='blue', size = 0.7) +
    geom_area(aes(fill = 'red', alpha=0.4), alpha = 0.3, position = 'identity', color = 'blue') +
    scale_fill_viridis(discrete = TRUE, begin=0.6, alpha=0.5, guide = FALSE) +
    xlab("False Positive Rate (1-Specifity)") +
    ylab("True Positive Rate (Sensitivity)") +
    geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0, 0.01)) +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0.01, 0)) + 
    theme_bw()
  
  roc_plot
}

createRocPlot(roc_obj_holdout, "ROC curve for best model (GBM)")

```

AUC provides an aggregate measure of performance across all possible classification thresholds. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0. In our case, the gbm model for the test set resulted in the value of 0.8733, what is a really good indication that our model performed really great predictions in the data test set. The graph bellow shows the AUC threshold trade-offs, where you can determine the TP rate and the FP rate for all the points inside the blue line.

## e) Inspect variable importance plots for the 3 models.

```{r}
# Inspect variable importance plots for the 3 models.

plot(varImp(model_rf), top = 5)

plot(varImp(model_gbm), top = 5)

plot(varImp(model_xgboost), top = 5)


```

Yes, the variables ended up being more less similar important for the RF, GBM and XGboost models. What is interesting to highlight are specially the variables (LoyalCH and PriceDiff), They have showed to be the first and second most important variables in all  of the 3 models, compounding the biggest part of the predictions contribution.

# Exercise 2

```{r, results='hide'}

# Clear memory
rm(list=ls())

my_seed <- (28081992)
h2o.init()

# Load the data

data <- as_tibble(ISLR::Hitters) %>%
  drop_na(Salary) %>%
  mutate(log_salary = log(Salary), Salary = NULL)
h2o_data <- as.h2o(data)


```

```{r}

skimr::skim(data)

```


## Quick analysis into the data-set

The dataset contains 3 factor variables, and 17 numeric ones , 263 observations. 
Non-missing values found (can tangibly reduce prediction power), with that no extra cleaning steps or imputation methods are necessary.
Log transformation applied on Salary (highly right tailed skewed distribuition) for better predictions results.

## a) Train 2 Random Forest models

```{r, results='hide'}
# A, Train 2 Random Forest models

y <- "log_salary"
X <- setdiff(names(h2o_data), y)

## 1, Random forest with 2 variables

rf_2_var <- h2o.randomForest(
  X, y,
  training_frame = h2o_data,
  model_id = "rf_2_var",
  mtries = 2,
  seed = my_seed
)

## 2, Random forest with 10 variables

rf_10_var <- h2o.randomForest(
  X, y,
  training_frame = h2o_data,
  model_id = "rf_2_var",
  mtries = 10,
  seed = my_seed
)

## Comparison of variable importance

h2o.varimp_plot(rf_2_var)
h2o.varimp_plot(rf_10_var)

```

Comparing the results of the models selecting 2 and 10 variables for each split, the features showed to be quite similar. The variables CatBat and CHits demonstrated to be the most important variables in both models.However, it's interesting to highlight that there a huge difference in scale of importance of those 2 variables in those 2 models.

## b) Explanation of extreme difference in variable importance

In our Random Forest Classifier, at each node of a decision tree, the features to be used for splitting the dataset is decided based on information gain(I.G.) or the more computationally cheap Gini impurity reduction. The CAtBat proved to be important for both models with 2 and 10 mtries, however with 10 randomly picked variables, the variable will be randomly selected more times by the trees (Default 50 trees) with that having its importance "increased in terms of scale.

## c) Two GBM models 

```{r, results='hide'}

# C, Two GBM models 

## GBM sample_rate = 0.1

gbm_srate_01 <- h2o.gbm(
  x = X, y = y,
  model_id = "gbm_srate_01",
  training_frame = h2o_data,
  sample_rate = 0.1,
  seed = my_seed
)

## GBM sample_rate = 1

gbm_srate_1 <- h2o.gbm(
  x = X, y = y,
  model_id = "gbm_srate_1",
  training_frame = h2o_data,
  sample_rate = 1,
  seed = my_seed
)

## Comparison of variable importance

h2o.varimp_plot(gbm_srate_01)
h2o.varimp_plot(gbm_srate_1)
# Close h2o session
h2o.shutdown()

```

The default sample rate value is 1, which means the bootstrap sample will have the same number of rows as the original data table. If you choose a value that is less than 1, then the bootstrap sample will have fewer rows that in the original table. As the bootstrapping always completely resampled the data building unrelated trees and with a sample_rate equal to 0.1, only 10% of the data gets replaced making the generated datasets look similar to each other, "enhancing"  in that way the other variables importance scale.

# Exercise 3

```{r , results='hide'}

rm(list=ls())

h2o.init()
#h2o.shutdown()
#h2o.no_progress()


my_seed <- (28081992)

# Load the data

data <- read_csv("D:/CEU/Data_Science_2/DS2_Yuri/HW1/data/KaggleV2-May-2016.csv")

# some data cleaning
data <- select(data, -one_of(c("PatientId", "AppointmentID", "Neighbourhood"))) %>%
  janitor::clean_names()

# for binary prediction, the target variable must be a factor + generate new variables
data <- mutate(
  data,
  no_show = factor(no_show, levels = c("Yes", "No")),
  handcap = ifelse(handcap > 0, 1, 0),
  across(c(gender, scholarship, hipertension, alcoholism, handcap), factor),
  hours_since_scheduled = as.numeric(appointment_day - scheduled_day)
)

# clean up a little bit
data <- filter(data, between(age, 0, 95), hours_since_scheduled >= 0) %>%
  select(-one_of(c("scheduled_day", "appointment_day", "sms_received")))

h2o_data <- as.h2o(data)


```

```{r}

skimr::skim(data)

```

## Quick analysis into the data-set

The datset contains 2 factor variables, and 16 numeric ones , 1070 observations.
Non-missing values found (can tangibly reduce prediction power), with that no cleaning steps or imputation methods are necessary.
Balanced response value no_show. (51417 No's and 20517 Yes), classification models can run smoothly without any further data preparation.

## a) Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts

```{r}
# A, Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts

splitted_data <- h2o.splitFrame(h2o_data, ratios = c(0.05, 0.45), seed = my_seed)
data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]

```

## b) Train rain a benchmark model of your choice and evaluate it on the validation set.

```{r, results='hide'}

# Glm: Elastic Net (Better Results)
y <- "no_show"
X <- setdiff(names(h2o_data), y)

glm_model <- h2o.glm(
  X, y,
  family = 'binomial',
  training_frame = data_train,
  model_id = "lm",
  nfolds = 5,
  seed = my_seed,
  keep_cross_validation_predictions = TRUE
)



```

```{r}

h2o.performance(glm_model, data_valid)

```


As the focus of the exercise is based on prediction, the most important metric to evaluate is the AUC (as no_show is balanced) that defines how well a binary classification model is able to distinguish between true positives and false positives. An AUC of 1 indicates a perfect classifier, while an AUC of .5 indicates a poor classifier, whose performance is no better than random guessing.On the following example (Elastid Net Model), you can visualize a really low AUC value (0.58) and the maximum accuracy (the number of correct predictions made as a ratio of all predictions made) that can be obtained as 0.51, this is just slightly better than random predictions.

## c) Build at least 3 models of different families using cross validation, keeping cross validated predictions.

```{r , results='hide'}

# C, Build at least 3 models of different families using cross validation, keeping cross validated predictions.
# You might also try deeplearning.

## Random forest

rf_model <- h2o.randomForest(
  X, y,
  training_frame = data_train,
  model_id = "rf",
  ntrees = 200,
  max_depth = 10,
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)


## GBM

gbm_model <- h2o.gbm(
  X, y,
  training_frame = data_train,
  model_id = "gbm",
  ntrees = 200,
  max_depth = 5,
  learn_rate = 0.1,
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

## Deep learning

deeplearning_model <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  model_id = "deeplearning",
  hidden = c(32, 8),
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

```


## d) Evaluate validation set performance of each model.

```{r}
# D, Evaluate validation set performance of each model.

# predict on validation set
validation_performances <- list(
  "glm" = h2o.auc(h2o.performance(glm_model, newdata = data_valid)),
  "rf" = h2o.auc(h2o.performance(rf_model, newdata = data_valid)),
  "gbm" = h2o.auc(h2o.performance(gbm_model, newdata = data_valid)),
  "deeplearning" = h2o.auc(h2o.performance(deeplearning_model, newdata = data_valid))
)

my_models <- list(
  glm_model, gbm_model, rf_model, deeplearning_model)

auc_on_validation <- map_df(my_models, ~{
  tibble(model = .@model_id, AUC = h2o.auc(h2o.performance(., data_valid)))
}) %>% arrange(AUC)

pander(auc_on_validation)

```

Based on the table analysis, it's possible to visualize that the LM, deeplearning and rf showed pretty similar AUC results with a slighter advantage to the linear model (the Winner) in this case. However, the selection process between them should also be based on the importance rates of identifying FP/FN. As the results were pretty similar it's important to investigate the optimal threshold defined for each one of those models based on those importance rates.

## e) How large are the correlations of predicted scores of the validation set produced by the base learners?

```{r}

h2o.model_correlation_heatmap(my_models, data_valid)

```

Based on the heating map, it's clear to perceive a stronger correlation between the deeplearning model with RF and LM. With that in consideration, I have decided not to use deeplearning on the final ensemble model. The other models demonstrated a correlation lower then 70-80% between them, which means that we can still use them in the final ensemble model.

## f) Create a stacked ensemble model from the base learners.

```{r, results='hide'}

# F, Create a stacked ensemble model from the base learners.

ensemble_models <- list(glm_model, gbm_model, rf_model)

ensemble_model <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = "glm",
  model_id = "stacked_model_glm",
  base_models = ensemble_models,
  seed = my_seed
)

```


## g) Evaluate ensembles on validation set. Did it improve prediction?

```{r}

auc_on_validation <- map_df(
  c(my_models, ensemble_model),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_valid)))}
)

pander(auc_on_validation)

```

Yes, at the end we could have improved slightly bit the previous glm model, however that improvement wasn't significant. The mechanism for improved performance with ensembles is often the reduction in the variance component of prediction errors made by the contributing models. As the models generated pretty similar results, it's possible that the benefits contracted from this variance component weren't really notable. 

## h) Evaluate the best performing model on the test set

```{r}

# How does performance compare to that of the validation set?

final_performance <- tibble(
  "ensemble_model_validation" = h2o.auc(h2o.performance(ensemble_model, newdata = data_valid)),
  "ensemble_model_test" = h2o.auc(h2o.performance(ensemble_model, newdata = data_test)),
)

pander(final_performance)


```

The results showed pretty much similar to each other (AUC indicators), there wasn't a significant difference in terms of performance between them.