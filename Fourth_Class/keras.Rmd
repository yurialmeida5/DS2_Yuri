---
title: "DS2 Lab5 - Digit classification on Kaggle"
subtitle: "Data Science 2: Machine Learning Tools - CEU 2021"
author: "Janos K. Divenyi, Jeno Pal"
date: '2020-03-29'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

```{r initialization}
library(tidyverse)
my_seed <- 20210329
```

## Data

```{r labelled-data}
data_with_labels <- read_csv("D:/CEU/Data_Science_2/DS2_Yuri/Fourth_Class/train.csv")
table(data_with_labels$label)
```

Let's plot our data.
```{r plot-images}
turnRowToMatrix <- function(data_row) {
  matrix(unlist(data_row), nrow = 28, byrow = TRUE) %>%
    apply(2, rev) %>%
    t()
}

showDigit <- function(data_row, label) {
  image(
    turnRowToMatrix(data_row),
    col = gray.colors(255), xlab = label, ylab = ""
  )
}

showFirst6Digits <- function(data, label) {
  par(mfrow = c(2, 3))
  if (missing(label)) label <- rep("", 6)
  walk(1:6, ~showDigit(data[., ], label[.]))
}

showFirst6Digits(data_with_labels[, -1], data_with_labels$label)
```

We have the same pixel data in test set but without labels.
```{r plot-test}
data_test <- read_csv("D:/CEU/Data_Science_2/DS2_Yuri/Fourth_Class/test.csv")
showFirst6Digits(data_test)
```


To build models, we need to separate the set with labels and use one of them for validation.
```{r data-split}
train_indices <- sample(seq(nrow(data_with_labels)), 10000)
data_train <- data_with_labels[train_indices, ]
data_valid <- data_with_labels[-train_indices, ]
```

## Sample submission
```{r silly-model}
sillyPredictor <- function(features) {
  rep(0, nrow(features))
}
calculateAccuracy <- function(prediction, y) {
  mean(prediction == y)
}
silly_predictions_valid <- sillyPredictor(data_valid)
calculateAccuracy(silly_predictions_valid, data_valid$label)
```

It might be interesting to look at misclassified samples
```{r silly-misclassified}
silly_misclass_samples <- silly_predictions_valid != data_valid$label
showFirst6Digits(data_valid[silly_misclass_samples, -1], silly_predictions_valid[silly_misclass_samples])
```

```{r sily-prediction}
sample_prediction <- tibble(imageId = seq(nrow(data_test)), label = sillyPredictor(data_test))
write_csv(sample_prediction, "submission0.csv")
```


## Benchmark model: simple tree
```{r tree}
set.seed(my_seed)
rpart_model <- rpart::rpart(
  factor(label) ~ .,
  data = data_train
)
treePredictor <- function(data) {
  max.col(predict(rpart_model, newdata = data))
}
tree_predictions_valid <- treePredictor(data_valid)
calculateAccuracy(tree_predictions_valid, data_valid$label)
```
```{r tree-prediction}
tree_prediction <- tibble(ImageId = seq(nrow(data_test)), Label = treePredictor(data_test))
write_csv(tree_prediction, "submission1.csv")
```

Let's examine the performance in more detail
```{r tree-misclassified}
tree_misclass_samples <- tree_predictions_valid != data_valid$label
showFirst6Digits(data_valid[tree_misclass_samples, -1], tree_predictions_valid[tree_misclass_samples])
```
```{r confusion-matrix}
plotConfusionMatrix <- function(label, prediction) {
  bind_cols(label = label, predicted = prediction) %>%
    group_by(label, predicted) %>%
    summarize(N = n()) %>%
    ggplot(aes(label, predicted)) +
      geom_tile(aes(fill = N), colour = "white") +
      scale_x_continuous(breaks = 0:9) +
      scale_y_continuous(breaks = 0:9) +
      geom_text(aes(label = N), vjust = 1, color = "white") +
      scale_fill_viridis_c() +
      theme_bw() + theme(legend.position = "none")
}
plotConfusionMatrix(data_valid$label, tree_predictions_valid)
```
```{r compare-good-and-bad}
label5 <- bind_cols(data_valid, prediction = tree_predictions_valid) %>%
  filter(label == 5)

showFirst6Digits(
  filter(label5, label != prediction) %>% select(-label, -prediction),
  filter(label5, label != prediction) %>% pull(prediction)
)

showFirst6Digits(
  filter(label5, label == prediction) %>% select(-label, -prediction)
)
```


## Deep neural nets with `keras`

The [homepage](https://keras.rstudio.com/) has great descrpitions, expamples
and tutorials. Cheatsheet [here](https://www.rstudio.com/resources/cheatsheets/).

```{r}
# Run this for the first time
# install.packages("keras")
# library(keras)
# install_keras()
```


```{r}
library(keras)
# The keras package automatically finds your python. You can control which version to use if you have multiple ones installed.
# use_python("/usr/local/bin/python3")
```



### A fully connected network example

Similar to what we saw with `h2o`. Keras need some restructure:

```{r adjust-data}
# Separate x & rescale
data_train_x <- as.matrix(select(data_train, -label)) / 255
data_valid_x <- as.matrix(select(data_valid, -label)) / 255
data_test_x <- as.matrix(data_test) / 255

# Separate y & one-hot encoding
data_train_y <- to_categorical(data_train$label, 10)
data_valid_y <- to_categorical(data_valid$label, 10)
```

```{r keras-model}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r keras-model-summary}
summary(model)
# 1000480 = 784 (input features) * 128 (first layer nodes) + 128 (biases)
# 1290 = 10 (output nodes) * 128 (first layer) + 10 (biases)
```

```{r keras-model-optimization-setting}
# Loss is optimized during the training, performance is evaluated based on the metric
# The metric itself is not necessarily smooth so it might be not a good idea to directly optimize for that
# compile modifies the model in place
compile(
  model,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r keras-model-training}
fit(
  model, data_train_x, data_train_y,
  epochs = 30, batch_size = 128,
  validation_data = list(data_valid_x, data_valid_y)
)
```

```{r keras-model-evaluation}
evaluate(model, data_valid_x, data_valid_y)
```

```{r keras-predictions}
keras_prediction <- tibble(ImageId = seq(nrow(data_test)), Label = predict_classes(model, data_test_x))
write_csv(keras_prediction, "submission2.csv")
```


```{r keras-confusion-matrix}
keras_predictions_valid <- predict_classes(model, data_valid_x)
plotConfusionMatrix(data_valid$label, keras_predictions_valid)
```
Compare predictions to reality:
```{r keras-good-and-bad}
label5 <- bind_cols(data_valid, prediction = keras_predictions_valid) %>%
  filter(label == 5)

showFirst6Digits(
  filter(label5, label != prediction) %>% select(-label, -prediction),
  filter(label5, label != prediction) %>% pull(prediction)
)

showFirst6Digits(
  filter(label5, label == prediction) %>% select(-label, -prediction)
)
```


### A convolutional neural net example

It makes use of the 2d structure of the original input data, applying
filters exploiting the 2d images. In `h2o` there is no option to use such models
by default.

```{r reshape-for-conv}
data_train_x <- array_reshape(data_train_x, c(nrow(data_train_x), 28, 28, 1))
data_valid_x <- array_reshape(data_valid_x, c(nrow(data_valid_x), 28, 28, 1))
data_test_x <- array_reshape(data_test_x, c(nrow(data_test_x), 28, 28, 1))
```

```{r cnn-definition}
cnn_model <- keras_model_sequential()
cnn_model %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r cnn-summary}
summary(cnn_model)
```

Number of parameters:
- `layer_conv_2d` turns 28 x 28 to 26 x 26, using 9 parameters for each filter (3 x 3 weights), plus a bias for each filter, altogether 320 parameters
- `max_pooling2d` takes each disjoint 2 x 2 squares and collapses them to 1, turning a 26 x 26
"image" to a 13 x 13. No parameters are associated with this step.
- `flatten`: turns each "pixel" in each node to one separate node: 13 x 13 x 32 = 5408
- `dense`: fully connected layer: 5408 nodes x 16 new nodes + 16 biases = 86544
- final fully connected layer: 16 x 10 + 10 = 170


```{r cnn-setup}
compile(
  cnn_model,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r cnn-train}
fit(
  cnn_model, data_train_x, data_train_y,
  epochs = 30, batch_size = 128,
  validation_data = list(data_valid_x, data_valid_y)
)
```

```{r keras-predictions}
cnn_prediction <- tibble(ImageId = seq(nrow(data_test)), Label = predict_classes(cnn_model, data_test_x))
write_csv(cnn_prediction, "submission3.csv")
```



### Data augmentation

You can increase your training sample size and sharpen your model with slightly
modifying your training sample data points, retaining the labels.

Set up steps with which we can alter images a bit:
```{r data-augmentation}
batch_size <- 128

train_datagen <- image_data_generator(
  rotation_range = 20
  # width_shift_range = 0.1,
  # height_shift_range = 0.1,
  # shear_range = 0.1,
  # zoom_range = 0.1
)

valid_datagen <- image_data_generator()

train_generator <- flow_images_from_data(
  x = data_train_x,
  y = data_train_y,
  generator = train_datagen,
  batch_size = batch_size
)

valid_generator <- flow_images_from_data(
  x = data_valid_x,
  y = data_valid_y,
  generator = valid_datagen,
  batch_size = batch_size
)
```

```{r cnn-with-augmentation}
cnn_model_w_augmentation <- keras_model_sequential()
cnn_model_w_augmentation %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model_w_augmentation,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

fit_generator(
  cnn_model_w_augmentation,
  train_generator,
  epochs = 10,
  steps_per_epoch = nrow(data_train_x) / batch_size,  # this does not make a difference here -- batch_size of the generator determines how training works
  validation_data = valid_generator,
  validation_steps = nrow(data_valid_x) / batch_size
)
```

```{r keras-predictions}
cnn_augmented_prediction <- tibble(ImageId = seq(nrow(data_test)), Label = predict_classes(cnn_model_w_augmentation, data_test_x))
write_csv(cnn_augmented_prediction, "submission4.csv")
```

### Transfer learning

If your problem is similar to another one, you might use an already trained model (like the models trained on the famous IMAGENET dataset) to spare time. General patterns in images are common and this knowledge can be "transferred".

Here, I just illustrate how transfer learning works. If you face a more general image recognition problem (e.g. dog vs cat) with less samples, transfer learning might help more.

```{r reshape-for-imagenet}
reshapeForTransferLearning <- function(x) {
  x3D <- replicate(3, array_reshape(x, c(nrow(x), 28, 28)))
  resized_x <- lapply(seq(nrow(x3D)), function(i) image_array_resize(x3D[i,,,], 32, 32))
  do.call(abind::abind, list(resized_x, along = 0))
}

data_train_x_reshaped <- reshapeForTransferLearning(data_train_x)
dim(data_train_x_reshaped)
data_valid_x_reshaped <- reshapeForTransferLearning(data_valid_x)
```

```{r use-imagenet-model}
imagenet_model <- application_densenet121(
  input_shape = c(32, 32, 3), weights = 'imagenet', include_top = FALSE
)
```

```{r transfer-model-setup}
transfer_model <- keras_model_sequential() %>%
  imagenet_model %>%
  layer_flatten()  %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  transfer_model,
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy")
)
summary(transfer_model)
```

```{r freeze-params}
freeze_weights(imagenet_model, to = "conv5_block16_2_conv")
summary(transfer_model)
```

```{r train-tranfer-model}
fit(
  transfer_model, data_train_x_reshaped, data_train_y,
  epochs = 5, batch_size = 128,
  validation_data = list(data_valid_x_reshaped, data_valid_y)
)
```
